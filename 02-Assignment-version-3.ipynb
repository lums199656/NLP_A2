{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_QLW_PATH = './data/qlw-train.json'\n",
    "TRAIN_RUMOUR_PATH = './data/train.json'\n",
    "DEV_PATH = './data/dev.json'\n",
    "TEST_PATH = './data/test-unlabelled.json'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. 导入的的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def import_from_json(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "        data_text = []\n",
    "        labels = []\n",
    "        for key in data_dict.keys():\n",
    "            data_text.append(data_dict[key]['text'].lower()) \n",
    "            try:\n",
    "                labels.append(int(data_dict[key]['label']))  \n",
    "            except KeyError:\n",
    "                pass\n",
    "        return data_text, labels\n",
    "\n",
    "def import_from_qlw(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data_dict = json.load(json_file)\n",
    "        data_text = []\n",
    "        labels = []\n",
    "        for key in data_dict.keys():\n",
    "            data_text.append(data_dict[key].lower()) \n",
    "            try:\n",
    "                labels.append(0)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return data_text, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 导入网上爬的不是谣言的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度为:  2786\n"
     ]
    }
   ],
   "source": [
    "train_nonrumour, train_nonrumour_label = import_from_qlw(TRAIN_QLW_PATH)\n",
    "print('训练集长度为: ', len(train_nonrumour))\n",
    "# print(train_nonrumour[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 导入老师给是谣言的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集长度为:  1168\n"
     ]
    }
   ],
   "source": [
    "train_rumour, train_rumour_label= import_from_json(TRAIN_RUMOUR_PATH)\n",
    "print('训练集长度为: ', len(train_rumour))\n",
    "# print(train_rumour[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 导入测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集长度为:  1410\n"
     ]
    }
   ],
   "source": [
    "test, _ = import_from_json(TEST_PATH)\n",
    "print('测试集长度为: ', len(test))\n",
    "# print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 导入开发集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开发集长度为:  100\n"
     ]
    }
   ],
   "source": [
    "dev, dev_label = import_from_json(DEV_PATH)\n",
    "print('开发集长度为: ', len(dev))\n",
    "# print(dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 去除符号 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长度: 5464\n",
      "符号去除完成！\n"
     ]
    }
   ],
   "source": [
    "all_ = train_rumour + train_nonrumour + dev + test\n",
    "print('长度:', len(all_))\n",
    "puncs = ['“', '”', '‘', '’', '–', '—', '...', '‐', '\\u200b', '.\\u2009.\\u2009.', '\\uf0b7', '\\uf020', '\\u200e', '\\u2066', \n",
    "               '\\u2069', '..', '. .', ',', '.', '\"', '\\u2018', '\\u00a0', '\\u2019']\n",
    "for index_ in range(len(all_)):\n",
    "    for punc in puncs:\n",
    "        all_[index_] = all_[index_].replace(punc,'')\n",
    "\n",
    "label = train_rumour_label + train_nonrumour_label + dev_label\n",
    "print('符号去除完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 去除 stopword + 去除非文字内容 - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理中 ...\n",
      "['houston', 'flooding', 'isnt', 'sign', 'climate', 'change', 'distinguished', 'us', 'climate', 'scientist', 'dr', 'roy', 'spencer', 'writes', 'context', 'climate', 'change', 'seeing', 'houston', 'new', 'level', 'disaster', 'becoming', 'common', 'flood', 'disaster', 'unfolding', 'houston', 'certainly', 'unusual', 'natural', 'weather', 'disasters', 'always', 'occurred', 'always', 'occurmajor', 'floods', 'difficult', 'compare', 'throughout', 'history', 'ways', 'alter', 'landscape', 'example', 'cities', 'like', 'houston', 'expand', 'years', 'soil', 'covered', 'roads', 'parking', 'lots', 'buidings', 'water', 'rapidly', 'draining', 'rather', 'soaking', 'soil', 'population', 'houston', 'ten', 'times', '1920s', 'houston', 'metroplex', 'expanded', 'greatly', 'water', 'drainage', 'basically', 'direction', 'downtown', 'houston']\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"处理中 ...\")\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english')) \n",
    "\n",
    "data_w2v = []\n",
    "for line in all_:\n",
    "    line_split = tt.tokenize(line)\n",
    "    line_tmp = copy.deepcopy(line_split)\n",
    "    for word in line_split:\n",
    "        if re.match('\\w+', str(word), flags=0) is None:\n",
    "            try:\n",
    "                line_tmp.remove(word)\n",
    "            except ValueError:\n",
    "                print(\"Not Found \", word)\n",
    "        for stopword in stopwords:\n",
    "            if word == stopword:\n",
    "                line_tmp.remove(word)\n",
    "    data_w2v.append(line_tmp)\n",
    "# print(data_w2v[0])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 预处理以及计算词频 - Word2Vec忽略这步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_rumour_list! well done!\n",
      "preprocessed_nonrumour_list! well done!\n",
      "preprocessed_dev_list! well done!\n",
      "preprocessed_test_list! well done!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemma_process(word_token):\n",
    "    word_token_pos_tag = nltk.pos_tag([word_token])\n",
    "    for word_token, pos_tag in word_token_pos_tag:\n",
    "        word_token_lower = word_token.lower()\n",
    "        wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "        if wordnet_pos == None:\n",
    "            word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet.NOUN)\n",
    "        else:\n",
    "            word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet_pos)\n",
    "    return word_token_lemma\n",
    "\n",
    "def calculate_frequency(data):\n",
    "    frequency_list = []\n",
    "    for value in data:\n",
    "        text = value.replace('\\n', ' ')\n",
    "        frequency_dict = dict()\n",
    "        stopword_removal = []\n",
    "        for word_token in tt.tokenize(text.lower()):\n",
    "            word_token_lemma = lemma_process(word_token)\n",
    "            if (word_token_lemma not in punc) and (word_token_lemma not in chinesepunc) and (word_token_lemma not in \n",
    "                stopword_list) and (word_token_lemma not in letter):\n",
    "                stopword_removal.append(word_token_lemma)\n",
    "        for stopword in stopword_removal:\n",
    "            if stopword in frequency_dict:\n",
    "                frequency_dict[stopword] += 1\n",
    "            else:\n",
    "                frequency_dict[stopword] = 1\n",
    "        frequency_list.append(frequency_dict)\n",
    "    return frequency_list # , word_list\n",
    "    \n",
    "tt = TweetTokenizer()\n",
    "\n",
    "stopword_list = set(stopwords.words('english'))\n",
    "punc = string.punctuation\n",
    "chinesepunc = ['“', '”', '‘', '’', '–', '—', '...', '‐', '\\u200b', '.\\u2009.\\u2009.', '\\uf0b7', '\\uf020', '\\u200e', '\\u2066', \n",
    "               '\\u2069', '..', '. .']\n",
    "letter = ['a', 'b', 'c', 'd', \"e\", \"f\", 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \n",
    "          'x', 'y', 'z'] \n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "preprocessed_rumour = calculate_frequency(train_rumour)\n",
    "print('preprocessed_rumour_list! well done!')\n",
    "preprocessed_nonrumour = calculate_frequency(train_nonrumour)\n",
    "print('preprocessed_nonrumour_list! well done!')\n",
    "preprocessed_dev = calculate_frequency(dev)\n",
    "print('preprocessed_dev_list! well done!')\n",
    "preprocessed_test = calculate_frequency(test)\n",
    "print('preprocessed_test_list! well done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 向量化 - Word2Vec忽略这步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3954, 76993) (1410, 76993) 3954\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# get vector for train_data, development_data, train_label\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def get_data_vector(rumour_data, nonrumour_data, test_data):\n",
    "    vectorizer = DictVectorizer()\n",
    "    transformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "    total_train = []\n",
    "    train_label = []\n",
    "    for rumour in rumour_data:\n",
    "        total_train.append(rumour)\n",
    "        train_label.append(1)\n",
    "    for nonrumour in nonrumour_data:\n",
    "        total_train.append(nonrumour)\n",
    "        train_label.append(0)\n",
    "    train_metrix = vectorizer.fit_transform(total_train)\n",
    "    test_metrix = vectorizer.transform(test_data)\n",
    "    train_vector = transformer.fit_transform(train_metrix)\n",
    "    test_vector = transformer.transform(test_metrix)\n",
    "    return train_vector, test_vector, train_label\n",
    "\n",
    "train, test, train_label = get_data_vector(preprocessed_rumour, preprocessed_nonrumour, preprocessed_test)\n",
    "print(train.shape, test.shape, len(train_label))\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Word2Vec 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x12ccc4130>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  gensim.models import Word2Vec\n",
    "\n",
    "Word2Vec(sentences=None,  #sentences可以是分词列表，也可以是大语料\n",
    "        size=200,#特征向量的维度\n",
    "        alpha=0.025,#学习率\n",
    "        window=5,#一个句子内，当前词和预测词之间的最大距离\n",
    "        min_count=3,#最低词频\n",
    "        max_vocab_size=None,#\n",
    "        sample=0.001, #随机下采样的阈值\n",
    "        seed=1,#随机数种子\n",
    "        workers=4,#进程数\n",
    "        min_alpha=0.0001,#学习率下降的最小值\n",
    "        sg=0, #训练算法的选择，sg=1，采用skip-gram，sg=0，采用CBOW\n",
    "        hs=0,# hs=1,采用hierarchica·softmax，hs=10,采用negative sampling\n",
    "        negative=5,#这个值大于0，使用negative sampling去掉'noise words'的个数（通常设置5-20）；为0，不使用negative sampling\n",
    "        cbow_mean=1,#为0，使用词向量的和，为1，使用均值；只适用于cbow的情况\n",
    "        iter = 5,#迭代次数\n",
    "        null_word = 0,\n",
    "        trim_rule = None, #裁剪词汇规则，使用None（会使用最小min_count）\n",
    "        sorted_vocab = 1,#对词汇降序排序\n",
    "        batch_words = 10000,#训练时，每一批次的单词数量\n",
    "        compute_loss = False,\n",
    "        callbacks = ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Word2Vec(vocab=133641, size=500, alpha=0.025)\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "#打印日志\n",
    "# logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',level=logging.INFO)\n",
    "#设置参数\n",
    "num_features = 500 #Word vector dimensionality\n",
    "min_word_count = 1 #\n",
    "num_workers = 4 #number of threads to run in parallel\n",
    "context = 10\n",
    "downsampling = 1e-3 #Downsample setting for frequent words\n",
    " \n",
    "#初始化和训练模型\n",
    "from gensim.models import word2vec\n",
    "print('Training model...')\n",
    "model = word2vec.Word2Vec(data_w2v,workers=num_workers,size=num_features,min_count= min_word_count,\n",
    "                          window = context,sample = downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name = '300features_40minwords_10context'\n",
    "#保存模型，以便下次使用或者继续训练\n",
    "model.save(model_name)\n",
    "print(model)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 叠加特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-0845e9a5704b>:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp_vector += np.array(model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.48810518e+01  1.58107160e+01 -3.65508498e+01  4.33990656e+00\n",
      "  7.34604349e-01 -1.39037077e+01  1.88867703e+00  1.49692652e+01\n",
      "  7.78252502e+00  1.24737095e+01 -8.57881704e+00 -3.39123804e+00\n",
      " -4.72216342e+00  4.50229942e+00 -2.10437179e+01 -8.88675542e+00\n",
      " -1.07999345e+01  1.15242196e+01  3.77096729e+00  1.15474597e+01\n",
      " -4.70896056e+00 -1.38330012e+00 -7.57880453e+00  2.93278698e+01\n",
      " -6.50483945e+00  1.01466720e+01 -2.12065998e+01  7.38376442e-01\n",
      " -2.78989696e+01 -1.51044969e+01 -2.13757751e+01 -2.12411297e+00\n",
      " -4.29281308e-01 -1.25552405e+01 -8.29708270e+00  1.62017031e+01\n",
      "  1.77938296e+01 -9.80730609e+00 -2.20074433e+01  5.46868805e+00\n",
      " -8.53115940e+00 -7.51626196e+00 -6.71424147e+00 -6.33084273e+00\n",
      "  1.17008817e+00  6.36960573e+00  4.02695874e-01 -5.18053748e+00\n",
      " -1.54149718e+01 -1.65095117e+01  1.29232609e+01 -3.17786559e-01\n",
      "  1.30894293e+01 -4.12434935e+00 -4.14448552e+00 -1.18720288e+01\n",
      " -3.12328275e+01  2.58260377e+01 -1.39281683e+01 -8.92473867e+00\n",
      " -8.58979710e+00  2.09819008e+01  7.41508897e+00 -3.67571075e+00\n",
      " -1.38831066e+01 -5.34369613e+00 -7.03909934e+00 -1.01867889e+01\n",
      " -9.96362646e+00  6.33559676e+00 -2.02149809e+01  5.46284671e+00\n",
      " -8.16825699e+00 -1.36787759e+01 -3.49285432e+01  3.39564958e+01\n",
      " -2.94048951e+01  7.39697042e+00  2.01786523e+00 -4.95993878e+00\n",
      "  7.41665540e+00 -1.28572693e+01 -1.19125347e+01  2.51698566e+01\n",
      " -1.94652402e+01  1.26843727e+01  3.98816920e+00  1.31184292e+00\n",
      " -4.19978926e+00 -1.17845300e+01  1.90602428e+00 -3.05098494e+00\n",
      " -3.62275265e+00 -2.57032759e+01 -3.66929295e+00 -9.99052158e+00\n",
      "  4.97390327e-01  1.01026137e+01  7.35008160e+00  9.21221001e-01\n",
      " -1.00106053e+01  1.11086508e+01 -3.68210525e+00 -3.99712077e+01\n",
      "  4.74429667e+00  1.72494462e+01  1.78063618e+01 -1.77394283e+01\n",
      "  4.99314339e+00  1.28818795e+01 -1.94059337e+01 -2.35793840e+01\n",
      "  8.59858896e+00 -1.99754340e+00  3.98928062e+00 -1.20655793e+01\n",
      "  1.77323118e+01  7.22145979e-01  2.27811283e+00  1.09008819e+01\n",
      "  1.60256187e+01  1.30383361e+01  2.19456177e+01 -2.19684673e+00\n",
      "  1.79577147e+01  1.83545822e+01 -6.07991087e+00 -1.67147462e+01\n",
      " -1.62542006e+01  3.23331182e+01 -1.96317724e+01  8.41889594e+00\n",
      "  6.36119893e+00 -2.43128026e+00 -7.57531421e-02 -1.35961938e+00\n",
      " -4.07147341e+00 -4.58295771e+00 -9.32302052e+00  1.48687588e+01\n",
      "  1.14094725e+01  1.59754209e+01 -9.40470452e+00 -9.18203575e+00\n",
      " -1.19645938e+01  9.18440584e+00 -1.49170367e+00 -2.47942023e+01\n",
      " -5.64710482e+01  1.00253489e+01  1.20555649e+01  1.85739094e+01\n",
      "  2.16159188e+01  3.89729481e+01  2.31523726e+01  1.20626293e+01\n",
      " -2.94922728e+01 -1.57317459e+01  1.87909882e+00 -5.76860715e+00\n",
      "  1.49058503e+01 -5.86113263e+00 -8.15048399e-01  1.56959215e+01\n",
      "  1.57064106e+01 -2.00751145e+01 -1.89448259e+01  4.04408977e+00\n",
      "  2.78329344e+01  7.51209165e+00  6.45440206e+00  1.92297514e+01\n",
      " -1.85955847e+00  8.48824860e+00  4.17770033e+00 -1.05785403e+01\n",
      "  1.09737487e+01  7.16713478e-01  7.64160630e+00 -3.70076275e+00\n",
      " -9.22816579e+00  2.11780193e+00  1.00950025e+01  3.69831308e+01\n",
      " -1.15284064e+01 -1.67590715e+01  7.18079687e+00  1.40773998e+00\n",
      " -8.29944673e+00 -2.18258360e+01 -1.89687838e-02 -2.76667090e+00\n",
      " -1.33929766e+01  1.71683961e+01  3.78837591e+01 -1.45268750e+01\n",
      " -5.57847670e+00 -2.68991102e+01  3.47052815e+00 -3.67363640e+00\n",
      "  1.05471160e+01 -6.98010963e+00  2.41883655e+01  1.77500230e+01\n",
      "  1.07380862e+01  1.10601805e+01  6.21066091e+00 -1.65746047e+01\n",
      " -1.85618841e+01  8.90097524e+00  3.07236756e+00 -1.53853685e+01\n",
      "  1.20662907e+01 -1.85999119e+01  9.47655654e+00  1.15794887e+00\n",
      "  1.02546086e+01 -3.29974075e+00  1.00785187e+01  2.47414972e+00\n",
      " -5.46592850e+00 -2.11606389e+01  1.13493529e+01  3.46315447e+00\n",
      " -9.21566307e+00 -2.97437838e+00  7.31009185e+00  1.13580798e+01\n",
      "  1.94743499e+00  9.35253106e+00  1.31510676e+01  1.44894284e+01\n",
      "  8.62394791e+00 -7.00337324e+00  3.85811133e+00  5.63435743e+00\n",
      " -7.44619080e+00  1.22034250e+01  2.74343721e+01 -5.15833111e+01\n",
      "  1.08585220e-01 -1.07583066e+00 -3.88788005e+00  2.58996956e+00\n",
      "  5.90113806e+00  1.47514427e+01  2.58604996e+01 -2.13771292e+01\n",
      "  3.03551392e+00  7.56826465e+00 -3.44780966e+00 -1.61799184e+01\n",
      "  5.66770683e+00 -2.85916783e+01  3.06398856e+01 -2.86823100e+00\n",
      " -4.35443849e+00 -2.04350695e+01 -1.61586297e+01 -1.94358870e+01\n",
      " -5.21671894e+00 -1.36859506e+01 -2.59652460e+01  1.46746577e+01\n",
      " -1.44882396e+01 -1.64883572e+01 -6.41003048e+00  1.83939853e+01\n",
      " -2.49279448e+01  1.99896432e+01 -4.35555442e+00 -1.17103589e+01\n",
      " -4.62354730e+00 -1.22203703e+01 -8.05844742e+00  7.94061172e+00\n",
      "  5.75198711e+00  8.01604750e+00  1.51543719e+00  5.56538396e-01\n",
      "  1.04729759e+00  3.51115571e+01 -1.00520258e+01  8.81332070e+00\n",
      " -3.81324757e+00  4.49356942e-01  3.00100619e+01  7.36002413e+00\n",
      "  1.06921019e+01  3.98087294e+00  1.44596234e+01 -3.64559928e+00\n",
      " -5.09929120e+00 -1.52017175e+01  1.54261847e+00  1.88576051e+01\n",
      "  5.58926164e+00 -6.20427071e+00  5.67772220e+00  1.46496194e+01\n",
      "  2.14535197e+00  1.11155279e+01 -1.60466162e+01  2.49441793e+01\n",
      "  3.64603140e-01 -1.27513079e+01  3.99111086e+00  2.92479940e+00\n",
      " -2.04909802e+01  9.63159428e+00 -7.79868069e+00  7.12313608e-01\n",
      "  8.88024741e+00  3.97727300e+01 -1.01081465e+01  5.40859392e+00\n",
      " -5.77306306e+00 -9.43913869e+00  3.21099902e+00 -2.94858746e+01\n",
      " -5.89971070e+00 -7.18254508e+00 -6.34365290e+00 -4.63312103e+00\n",
      "  9.38977308e+00  1.73400039e+01 -1.38454666e+01  2.22301977e+01\n",
      " -9.35771004e+00 -1.60691919e+01 -2.08544052e+01 -1.34388907e+01\n",
      "  1.00928770e+01 -1.37278527e+01  3.88252837e+00  1.31147563e+01\n",
      "  1.72955408e+01 -5.41953237e+00  2.54899090e+01  2.82410433e+01\n",
      " -1.64311390e+01 -1.73038787e+01 -4.50007014e+00  8.91929561e-01\n",
      "  5.36808032e+00  1.76489262e+00 -1.89782560e+00 -8.64839661e+00\n",
      "  7.60225555e+00 -2.47279258e+00  3.41371801e+01  4.99630047e+00\n",
      "  7.58853714e+00  8.70252742e+00  5.84141986e+00 -6.67601164e+00\n",
      "  2.02028384e+01  7.57826471e+00  1.27415326e+01  1.28991421e+01\n",
      "  9.89963893e+00 -3.65246628e+00 -2.72233182e+00  3.06342509e-01\n",
      "  9.91823617e-01 -6.73376785e+00  5.46315633e+00 -2.69574889e+00\n",
      " -2.19148407e+01 -4.94542752e+00  3.47054167e+00 -1.00742126e+01\n",
      "  5.06375964e+00 -1.29166063e+01 -2.85532070e+01 -5.94042637e+00\n",
      " -1.85787054e+00  7.71773415e+00 -4.30335295e+00 -3.82793448e-01\n",
      " -1.90880147e+01  1.24169422e+01 -5.17443128e+00  6.49163736e+00\n",
      " -5.41032078e-01  2.00949776e+00 -3.22152145e-01  7.37945284e+00\n",
      "  7.06195878e+00 -3.38720118e+01 -6.54413945e+00  3.96689630e+00\n",
      "  4.91645788e+00  2.22998398e+00  1.07751976e+01 -2.87182281e+01\n",
      " -2.04532754e+01  6.19501787e+00  2.71760168e+00 -1.58553192e+00\n",
      " -1.30339259e+00 -1.49672187e+01 -6.55357752e+00 -1.18727173e+01\n",
      "  3.53005413e-01  3.00892897e+01 -3.21706974e+00 -1.53451892e+01\n",
      " -9.34328345e+00  8.52295668e+00 -6.76111388e+00 -1.71015822e+00\n",
      "  2.84925775e-01 -3.15510978e+01  1.85150652e+01 -4.88116330e+00\n",
      "  4.42711568e+00  1.97073201e+01  2.72270067e+01  7.32368919e+00\n",
      " -5.55376901e+00  2.34836657e+01  3.06396524e-01  1.48214407e+01\n",
      "  1.96480426e+01  2.08041888e+01 -8.25096486e+00 -5.66773797e+00\n",
      "  2.10607111e+00 -2.73843186e-01  9.33407425e+00 -1.37669131e+01\n",
      "  8.13390852e+00  9.83290933e+00 -1.76138523e+01 -1.95608883e+01\n",
      "  2.10458260e+01  1.30118550e+01  6.35403458e+00 -7.67334168e+00\n",
      " -2.42656504e+01  3.53538362e+00  1.84800161e+00  2.50999591e+01\n",
      "  8.06546820e+00  7.95641907e+00 -3.00456426e+01 -6.47008436e+00\n",
      "  2.12253534e+00  8.94508556e+00  2.11944766e+00  7.43713722e+00\n",
      "  1.43602400e+01  3.74243961e+00  1.20031957e+01  1.43802245e+01\n",
      " -9.01236822e+00  1.84872363e+01  1.09449810e+01  1.05219134e+01\n",
      "  2.46034597e+00 -1.36334705e+01  1.38139198e+01 -6.28673431e+00\n",
      "  2.65051101e+01 -1.80421180e+01 -3.83863731e+00  1.83453668e+01\n",
      "  1.47474887e+01 -9.76721592e+00 -3.31076892e-01  2.01419316e+01\n",
      "  1.23040643e+01 -1.43655807e+01 -1.89474823e+01 -2.52087733e+01\n",
      " -7.56041136e+00 -5.86129077e+00 -1.32221289e+01 -1.60529425e+01\n",
      " -2.69819717e+01  2.86447992e+01 -1.20224787e+01 -1.79653324e+01\n",
      " -7.47307187e+00 -4.78747073e+00 -7.40951402e+00 -1.25827672e+01\n",
      "  2.33408712e+01 -1.11489994e+01 -5.96823210e+00 -2.32776285e+01\n",
      "  1.65813942e+01  1.13337845e+01  8.36495559e+00  1.95859799e+01\n",
      " -4.29811340e+00  5.31084833e-01 -8.60464418e+00 -2.13941513e+01]\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Working ...')\n",
    "result = []\n",
    "\n",
    "for row in all_:\n",
    "    tmp_vector = np.array([0.0]*num_features)\n",
    "    for word in row:\n",
    "        try:\n",
    "            tmp_vector += np.array(model[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    result.append(tmp_vector)\n",
    "print(result[0])\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 监测数据准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集和label的长度: 3954 3954\n",
      "dev 长度: 100\n",
      "test 长度 1410\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train = data_w2v[:-(len(test)+100)]\n",
    "train_label = label[:-100]\n",
    "print('训练集和label的长度:', len(train), len(train_label))\n",
    "\n",
    "dev = result[-(len(test)+100):-len(test)]\n",
    "print('dev 长度:',len(dev))\n",
    "\n",
    "test = result[-len(test):]\n",
    "print('test 长度', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ...\n",
      "The F-Score is: 0.38694554633970435\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest ...')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Choose the type of classifier. \n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [4, 6, 9], \n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2, 3, 5, 10], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,5,8]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(train, train_label)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "clf.fit(train, train_label)\n",
    "Y_pred = clf.predict(dev)\n",
    "print ('The F-Score is:', f1_score(dev_label, Y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=[5000],activation='logistic',solver ='adam',random_state=3,max_iter=1000)\n",
    "mlp.fit(train, train_label)\n",
    "Y_pred = mlp.predict(test)\n",
    "print('DONE!')\n",
    "# print ('The F-Score is:', f1_score(dev_label, Y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "TEST_LABEL_PREDICTION_PATH = './output/test-output.json'\n",
    "\n",
    "def save_test_label(prediction_list, path):\n",
    "    test_total_labels_dict = dict()\n",
    "    num = 0\n",
    "    for test_label in prediction_list:\n",
    "        label_dict = dict()\n",
    "        label_dict['label'] = test_label\n",
    "        test_total_labels_dict['test-'+str(num)] = label_dict\n",
    "        num = num + 1\n",
    "    json_str = json.dumps(test_total_labels_dict)\n",
    "    with open(path, 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "Y_pred = Y_pred.tolist()\n",
    "save_test_label(Y_pred, TEST_LABEL_PREDICTION_PATH)\n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bita6bf41cde9924fd7ba0d50535d03cb30"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}